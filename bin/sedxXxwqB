#!/usr/bin/env bash
set -euo pipefail

# Bootstrap Script Post-Deployment Monitoring
# Tracks usage patterns, errors, and user satisfaction
# Implements product launch monitoring principles

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Configuration
MONITOR_LOG="$PROJECT_ROOT/.bootstrap_monitor.log"
USAGE_STATS="$PROJECT_ROOT/.bootstrap_usage.json"
ERROR_LOG="$PROJECT_ROOT/.bootstrap_errors.log"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Utilities
log() { echo -e "${BLUE}[MONITOR]${NC} $*"; }
warn() { echo -e "${YELLOW}[WARN]${NC} $*"; }
error() { echo -e "${RED}[ERROR]${NC} $*"; }
success() { echo -e "${GREEN}[SUCCESS]${NC} $*"; }

# Record bootstrap execution
record_execution() {
    local exit_code="$1"
    local flags="$2"
    local timestamp=$(date -Iseconds)
    local duration="${3:-0}"

    # Create monitoring log entry
    cat >> "$MONITOR_LOG" <<EOF
TIMESTAMP: $timestamp
EXIT_CODE: $exit_code
FLAGS: $flags
DURATION: ${duration}s
---
EOF

    # Update usage statistics (JSON format for easy parsing)
    update_usage_stats "$exit_code" "$flags" "$timestamp"

    log "Execution recorded: exit_code=$exit_code, flags='$flags', duration=${duration}s"
}

# Update usage statistics
update_usage_stats() {
    local exit_code="$1"
    local flags="$2"
    local timestamp="$3"

    # Initialize stats file if it doesn't exist
    if [[ ! -f "$USAGE_STATS" ]]; then
        cat > "$USAGE_STATS" <<'EOF'
{
  "total_executions": 0,
  "successful_executions": 0,
  "failed_executions": 0,
  "common_flags": {},
  "error_patterns": {},
  "first_run": "",
  "last_run": "",
  "version": "2.0.0"
}
EOF
    fi

    # Update statistics using python for JSON manipulation
    python3 -c "
import json
import sys
from datetime import datetime

try:
    with open('$USAGE_STATS', 'r') as f:
        stats = json.load(f)

    # Update counters
    stats['total_executions'] += 1
    if $exit_code == 0:
        stats['successful_executions'] += 1
    else:
        stats['failed_executions'] += 1

    # Track common flags
    flags = '$flags'
    if flags not in stats['common_flags']:
        stats['common_flags'][flags] = 0
    stats['common_flags'][flags] += 1

    # Update timestamps
    if not stats['first_run']:
        stats['first_run'] = '$timestamp'
    stats['last_run'] = '$timestamp'

    # Write back
    with open('$USAGE_STATS', 'w') as f:
        json.dump(stats, f, indent=2)

except Exception as e:
    print(f'Error updating stats: {e}', file=sys.stderr)
"
}

# Record error details
record_error() {
    local error_type="$1"
    local error_message="$2"
    local context="$3"
    local timestamp=$(date -Iseconds)

    cat >> "$ERROR_LOG" <<EOF
TIMESTAMP: $timestamp
ERROR_TYPE: $error_type
MESSAGE: $error_message
CONTEXT: $context
---
EOF

    warn "Error recorded: $error_type - $error_message"
}

# Generate usage report
generate_report() {
    local report_type="${1:-summary}"

    echo "ðŸ“Š Bootstrap Script Usage Report"
    echo "=================================="
    echo

    if [[ ! -f "$USAGE_STATS" ]]; then
        warn "No usage statistics available"
        return 0
    fi

    # Extract key metrics using python
    python3 -c "
import json
from datetime import datetime

try:
    with open('$USAGE_STATS', 'r') as f:
        stats = json.load(f)

    total = stats.get('total_executions', 0)
    successful = stats.get('successful_executions', 0)
    failed = stats.get('failed_executions', 0)

    print(f'Total Executions: {total}')
    print(f'Successful: {successful} ({100*successful/total:.1f}% if total > 0 else 0.0}%)')
    print(f'Failed: {failed} ({100*failed/total:.1f}% if total > 0 else 0.0}%)')
    print(f'Success Rate: {100*successful/total:.1f}% if total > 0 else 0.0}%')
    print()

    if '$report_type' == 'detailed':
        print('Common Flag Combinations:')
        flags = stats.get('common_flags', {})
        for flag_combo, count in sorted(flags.items(), key=lambda x: x[1], reverse=True)[:5]:
  